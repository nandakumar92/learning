# -*- coding: utf-8 -*-
"""data_analysis_with_pyspark_book_ch6-9.ipynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BLYDY5tCTq9p8dffLof4fZPptx0oNwH5
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

!ls

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark

shows=spark.read.json('/content/shows-silicon-valley.json')

shows.show(1)

shows.count()

shows.printSchema()

print(shows.columns)

array_subset=shows.select('name','genres')

# extracting data from columns
import pyspark.sql.functions as F
array_subset=array_subset.select('name',array_subset.genres[0])
array_subset.show(10)

## create map of two arrays
columns=['name','language','type']
shows_map=shows.select(*[F.lit(col) for col in columns],F.array(*columns).alias('values'))
shows_map.show(10)
shows_map.select('values').show(10)

import pyspark.sql.types as T
elements=spark.read.csv('/content/Periodic_Table_Of_Elements.csv',header=True,inferSchema=True)

elements.createOrReplaceTempView('elements')
spark.sql("select period,count(*) from elements group by 1")

## views tables in catalog
spark.catalog

spark.catalog.listTables()

spark.catalog.dropTempView('elements')

##using rdd's

collection=[1,'a',3.0,('five',5)]

sc=spark.sparkContext
collection_rdd=sc.parallelize(collection)
print(collection_rdd)

## rdd are collection of objects. you can use any lambda function on these objects.
def add_value(value):
  try:
    return value+1
  except:
    return value

collection_rdd.filter(lambda x:x.isinstance(x,(float,int)))
collection_rdd=collection_rdd.map(add_value)

## udf's

import pyspark.sql.functions as F
from pyspark.sql.types import IntegerType


columns = ["Seqno","Name"]
data = [("1", "john jones"),
    ("2", "tracey smith"),
    ("3", "amy sanders")]

df = spark.createDataFrame(data=data,schema=columns)

df.show(truncate=False)

def convertCase(str):
    resStr=""
    arr = str.split(" ")
    for x in arr:
       resStr= resStr + x[0:1].upper() + x[1:len(x)] + " "
    return resStr 
  
convertUDF = F.udf(lambda z: convertCase(z),StringType())

df.select("Seqno", \
    convertUDF("Name").alias("Name") ) \
   .show(truncate=False)

## window functions

netflix=spark.read.csv('/content/Netflix Revenue.csv',header=True,inferSchema=True)

netflix.show(10)

netflix.printSchema()

import pyspark.sql.functions as F
from pyspark.sql.types import DateType

netflix.withColumn('Date',F.col('Date').cast(DateType()))

netflix.filter('date is not null').show(10)
netflix.printSchema()

netflix_v2=netflix.groupBy('date').agg(F.sum('Global Revenue').alias('total_revenue')).orderBy('date',ascending=False)

from pyspark.sql.window import Window
cum_date=Window.orderBy('date')
netflix.withColumn('Global Revenue_cumulative',F.sum('Global Revenue').over(cum_date)).show(10)

churn=spark.read.csv('/content/Customer-Churn-Records.csv',header=True,inferSchema=True)

churn.show(10)

windowvar=Window.orderBy('CustomerId').rowsBetween(-5,Window.currentRow)
churn.withColumn('cumulative_balance',F.sum('Balance').over(windowvar)).show(10)


# -*- coding: utf-8 -*-
"""data_analysis_with_pyspark_book_ch4-5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-D3etNX1H_8U2L6_bRk5tVFnpHrBXXWQ

Learning and practice from book - https://www.amazon.com/Analysis-Python-PySpark-Jonathan-Rioux
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

!ls

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark

logs=spark.read.option('delimiter','|').option('header',True).option('inferSchema',True).option('timestampFormat',"yyyy-mm-dd").csv('/BroadcastLogs_2018_Q3_M8_sample.CSV')

logs.printSchema()

logs.show(10)

logs.select("LogServiceID","BroadcastLogID","LogDate").show(10)

## .columns provides a list of columns in the dataset
logs.columns

import numpy as np

columns_split=np.array_split(logs.columns,3)
columns_split

logs=logs.drop("BroadCastLogID","SequenceNO")

print("BroadCastLogID" in logs.columns)

logs.select('*').show(10)

from pyspark.sql.functions import col

logs=logs.withColumn("duration_seconds",col("Duration").substr(1, 2).cast("int")*60*60+ col("Duration").substr(4, 2).cast("int")*60+ col("Duration").substr(7, 2).cast("int")
                     )

logs.show(10)

logs.withColumnRenamed("duration_seconds_v2","duration_seconds").show(10)

logs.toDF(*[x.lower() for x in logs.columns]).show(10)

logs.printSchema()

logs.select(sorted(logs.columns)).show(10)

# using describe to show summary statistics.
for i in logs.columns:
  logs.describe(i).show()

## use summary to show percentile statistics
for i in logs.columns:
  logs.select(i).summary().show()

logs.describe('Language2')

logs.select('Language2').summary()

## use .columns to create new df with only columns having no "ID" in the col name.

def colx(lst):
  y=[]
  for i in lst:
    if "ID" not in i:
      y.append(i)
  return y

y=colx(logs.columns)
print(y)

logs_clean=logs.select([x for x in y])
logs_clean.show(10)

logs.printSchema()

cd_cateogry=spark.read.csv("/CD_Category.csv",inferSchema=True,sep='|',header=True)

cd_cateogry.select('*').show(10)

cd_age=spark.read.csv("/CD_AudienceTargetAge.csv",inferSchema=True,sep='|',header=True)

cd_age.select('*').show(10)

cd_cateogry.join(cd_age,on="ActiveFG",how="left").show(10)

log_identifier=spark.read.csv("/LogIdentifier.csv",inferSchema=True,sep='|',header=True)

logs_and_channels=logs.join(log_identifier,on="LogServiceId",how="inner")

cd_program_class=spark.read.csv("/CD_ProgramClass.csv",inferSchema=True,sep='|',header=True)
full_log=logs_and_channels.join(cd_cateogry,on="CategoryId",how="left").join(cd_program_class,"ProgramClassId",how="left")

full_log.select('*').show(10)

import pyspark.sql.functions as F
full_log.groupBy('ProgramClassCD').agg(F.sum('duration_seconds').alias('duration_total')).orderBy('duration_total',ascending=False).show(10)

## drop nulls - how -any - drop rows with any null, all - drop rows will all nulls. subset- list of columns to consider.
logs_no_null=logs.dropna(how='any',subset=['LogServiceID'])

logs.count()
logs_no_null.count()

logs.fillna(0).show(10)

